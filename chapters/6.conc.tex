This project presents an approach for the automatic recognition of transient events with the use of machine learning techniques. Given proposal was developed under the scope of forthcoming astronomical synaptic surveys such as the LSST (\cite{0805.2366}).

The method introduced in this project consists in oversampling filtered light curves, then extracting characteristic features from them, and finally using those features as inputs to machine learning algorithms. The features extracted from light curves were either statistical descriptors of the observations, or polynomial curve fitting coefficients applied to the light curves.  
Three different machine learning models were trained with the resulting measurements.

A variety of classification tasks were researched. Namely, binary classification among transients and non-transients, and multi-class classification of various transient classes, sometimes including non-transients too. Detailed description of these tasks can be found in Section \ref{section_method}. 
Overall, the best classifier for all tasks was the Random Forest, followed by Neural Networks and then Support Vector Machines.

For the sake of experimentation, each task was executed several times with different data subsets. Given subsets corresponded to the decision of selecting parameter value combinations, including: balancing classes or not, using either 30, 26 or 20 features only and linearly-scaling or standardizing feature vector values.

State of the art results were obtained when testing the trained models to unseen sets of data, specifically in binary and transient multi-class classification. Recall scores obtained from the best classifier for each task, are equivalent to those results found in \cite{1401.3211} and \cite{1601.03931}.


The best experimentation parameters were also investigated. 
Training with light curves which contained 10 observations minimum generally outperformed those with minimum 5. Overall, the best results were also achieved using all 30 features per light curve, while using 20 features overall performed worst. Such phenomena may be explained due to the lower noise that curves with more observations provide.
Since light-curves with higher observation count may be better defined, they could generate a more precise higher rank polynomial fitting that non-transients, which were the newly proposed features found in the 26 and 30 feature sets.
In general, training with unbalanced-class data sets resulted in the highest values. Contrary to what was expected, the usage of oversampled light-curves may be a factor that biases the model during training. This could be explained if such artificial light curves were too similar to the original ones, and thus the algorithms over-fitted during training.

Feature importance was also studied based on the best performing models for each task (Random Forests). An analysis resulted in the identification of the most relevant and least important features regardless of the experiment in which they were used. In detail, the best feature was always stetson\textunderscore j, followed by: amplitude, standard deviation, skewness and median absolute deviation. Conversely, 4th level polynomial fitting coefficients were the ones which provided the least relevance, though using them was still better, since the highest scores were a result of using all 30 features. Furthermore, lower polynomial fitting features like poly2\textunderscore t1 and poly1\textunderscore t1 were proven to be beneficial for classification, scoring much a higher importance.

The methodology proposed in this project, together with the positive results lay the foundations on the development of more robust Transient Object Classifiers. This research becomes a base for the work of Research groups at Universidad de los Andes and CPPM keep exploring the field, groups that also look forward to contribute to the new age of synoptic surveys.


% ------------------------------------------------------------

\section{Future Work} \label{future_work}

Though the obtained results demonstrate that the methodology proposed works well, final scores are far from perfect. Results are not significantly better than what the state of the art already obtained, meaning there are still more improvements to develop in order to construct better classifiers which are usable in next-generation astronomical surveys. 

Finding clean information is one of the hardest tasks when using photometric data. Astronomical data-sources tend to be obscure and hard to deal with, which makes it difficult to understand the information that is being gathered. Having said that, a critical improvement on transient recognition would be to train the algorithms with cleaner data. This means data with lower amount of duplicates and errors, and specifically, using a more reliable non-transient data-set. 

Expanding the methodology presented may be beneficial too. Using additional features could also increase the classifiers performance. Moreover, different classification algorithms can also be tested for a better detection of transient objects.

Finally, testing with other more reliable oversampling types would be beneficial for the purpose of working with limited data-sets. While the oversampled/balanced inputs increased the performance of some of the algorithms, different alternatives could improve upon the results shown in this document.
