Our methodology has six stages: 
filtering out irrelevant light curves, oversampling light curves, extracting
descriptive features from the curves, processing them into feature
vectors, re-scaling those resulting feature vectors, and 
classification with machine learning algorithms. 
The details of each step are in the following subsections.

\subsection{Data Filtering} \label{subsection_filtering}

We discard light curves with few observations as they may not contain
enough information to be classified correctly by machine learning
algorithms.  
The nominal cut is 5 observations per light curve, but we also test a
higher cut by filterig out light curves with less than 10 observations.

\subsection{Oversampling Transient Light Curves} \label{subsection_oversampling}

The number light curves per class is unbalanced. 
In order to have the same amount elements for each class we implement an
oversampling step by artificially generating multiple mock light curves,
each based on an observed one. 

We generate a mock light curve from the observed light curve and 
then sample the observed magnitude from a Gaussian distribution
centered on the observational apparent magnitude with the magnitude's
error as the  standard deviation. 

{\bf Y entonces se generaron al final 1293 curvas para todos las
  clases para tener el mismo numero de elementos que la clase SN?}

\subsection{Feature Extraction} \label{subsection_extraction}

Light curves are sampled at irregular time intervals and have
different numbers of data points.
This makes it challenging to directly use the time-series data for
classification with traditional methods. 
We circumvent this problem by extracting a set of features for each
light curve.
These features are scalars dereive from statistical and model-specific
fitting techniques.
Some of these features were  were formally introduced in
\cite{1101.1959}, and have been used in other studies
\citep{1603.00882,1601.03931}.  

In total, 30 features were used in this project. These features were computed for all light curves, and can be classified according to their origin in four groups: moment-based, magnitude-based, percentile-based and fitting-based. The groups are presented next.

\begin{enumerate}
    
    \item Moment-based features use the magnitude for each light curve.
    \begin{itemize}
        \item \underline{Beyond1std} (\textit{beyond1std}): 
        Percentage of observations which are over or under one standard deviation from the weighted average. Each weight is calculated as the inverse of the corresponding observation's photometric error.
        \item \underline{Kurtosis} (\textit{kurtosis}): 
        The fourth moment of the data distribution. Used to measure the heaviness or lightness in the tails of the statistical data.
        \item \underline{Skewness} (\textit{skew}): 
        A measurement of the level of asymmetry from the normal distribution in a data distribution. Negative skewness is the property of a more pronounced left tail, while positive skewness is a characteristic that implies a more pronounced right tail.
        \item \underline{Small Kurtosis} (\textit{sk}):
        Small sample kurtosis.
        \item \underline{Standard deviation} (\textit{std}):
        The standard deviation of the magnitudes..
        \item \underline{Stetson J} (\textit{stetson\textunderscore j}):
        The Welch-Stetson J variability index \cite{1996PASP..108..851S}. A robust standard deviation.
        \item \underline{Stetson K} (\textit{stetson\textunderscore k}): 
        The Welch-Stetson K variability index \cite{1996PASP..108..851S}. A robust kurtosis measure.
    \end{itemize}
    
    \item Magnitude-based use the magnitude for each source.
    \begin{itemize}
        \item \underline{Amplitude} (\textit{amp}): 
        The difference between the maximum and minimum magnitudes.
        \item \underline{Max Slope} (\textit{max\textunderscore slope}): 
        Maximum absolute slope between two consecutive observations.
        \item \underline{Median Absolute Deviation} (\textit{mad}): 
        The median of the difference between magnitudes and the median magnitude.
        \item \underline{Median Buffer Range Percentage} (\textit{mbrp}): 
        The percentage of points within 10\% of the median magnitude.
        \item \underline{Pair Slope Trend} (\textit{pst}): 
        Percentage of all pairs of consecutive magnitude measurements that have positive slope.
        \item \underline{Pair Slope Trend 30} (\textit{pst\textunderscore last30}): 
        Percentage of the last 30 pairs of consecutive magnitudes that have a positive slope, minus percentage of the last 30 pairs of consecutive magnitudes with a negative slope.
    \end{itemize}
    
    \item Percentile-based features use the sorted flux distribution for each source. Flux is calculated as $F = 10^{0.4mag}$. Defining $F_{a,b}$ as the difference between the $b$ and $a$ flux percentiles.
    \begin{itemize}
        \item \underline{Percent Amplitude} (\textit{p \textunderscore amp}): 
        Largest percentage difference between the absolute maximum magnitude and the median.
        \item \underline{Percent Difference Flux Percentile} (\textit{pdfp}): 
        Ratio between $F_{5,95}$ and the median flux.
        \item \underline{Flux Percentile Ratio Mid20} (\textit{fpr20}): 
        Ratio $F_{40,60} / F_{5,95}$
        \item \underline{Flux Percentile Ratio Mid35} (\textit{fpr35}):
        Ratio $F_{32.5,67.5} / F_{5,95}$
        \item \underline{Flux Percentile Ratio Mid50} (\textit{fpr50}): 
        Ratio $F_{25,75} / F_{5,95}$
        \item \underline{Flux Percentile Ratio Mid65} (\textit{fpr65}): 
        Ratio $F_{17.5,82.5} / F_{5,95}$
        \item \underline{Flux Percentile Ratio Mid80} (\textit{fpr80}): 
        Ratio $F_{10,90} / F_{5,95}$
    \end{itemize}
    
    \item Polynomial Fitting-based features are the coefficients of multi-level terms in polynomial curve fitting. These is a new set of features proposed in this paper. They use the magnitude for each source.
    \begin{itemize}
        \item \underline{Poly1 T1}: Linear term coeff. in monomial curve fitting.
        \item \underline{Poly2 T1}: Linear term coeff. in quadratic curve fitting.
        \item \underline{Poly2 T2}: Quadratic term coeff. in quadratic curve fitting.
        \item \underline{Poly3 T1}: Linear term coeff. in cubic curve fitting.
        \item \underline{Poly3 T2}: Quadratic term coeff. in cubic curve fitting.
        \item \underline{Poly3 T3}: Cubic term coeff. in cubic curve fitting.
        \item \underline{Poly4 T1}: Linear term coeff. in quartic curve fitting.
        \item \underline{Poly4 T2}: Quadratic term coeff. in quartic curve fitting.
        \item \underline{Poly4 T3}: Cubic term coeff. in quartic curve fitting.
        \item \underline{Poly4 T4}: Quartic term coeff. in quartic curve fitting.
    \end{itemize}

\end{enumerate}

The computed features were grouped in the following sets for experimentation:

\begin{enumerate}
    \item \underline{20 feats:} This sub-set includes all moment-based, magnitude-based and percentile-based features.
    \item \underline{26 feats:} This sub-set includes all features, except quartic curve fitting parameters (poly4).
    \item \underline{30 feats:} This sub-set includes all moment-based, magnitude-based, percentile-based and fitting-based features.
\end{enumerate}


\subsection{Feature Scaling} \label{subsection_scaling}
Two feature scaling procedures were used in this project, in order to weight equally each feature when training machine learning algorithms. This process was applied independently to every single light curve feature. The first re-scaling procedure, named standardization, recalculates the values such that their updated mean is zero and they have unit variance. The second re-scaling feature used was normalization, which transforms features by re-scaling them to the range $[0,1]$. Note that these techniques were used exclusively in distinct experiments as explained in the experimentation section (Section \ref{section_experimentation}).

\subsection{Classification} \label{subsection_classification}

Five classification tasks were performed with the homogeneous computed feature vectors.

\begin{enumerate}
    \item \textbf{Binary Classification}: 
    Distinguish transients from non-transients. A balanced number of events from both classes is used to investigate the capability of distinguishing between these different types.
    \item \textbf{6-Transient Classification}: Recognize objects as belonging to one of the more common transient types in the dataset, namely: AGN, Blazar, CV, Flare, HPM and Supernovae. The purpose of this task was to test how well did classification algorithms perform when distinguishing only the main transient event types in the dataset.
    \item \textbf{7-Transient Classification}: Recognize objects as belonging to one of the more common transient types in the dataset or as a different transient, namely: AGN, Blazar, CV, Flare, HPM, Other and Supernovae. The Other class was created by using objects from ambiguous and under-represented classes such as the ones described in Section \ref{section_data}. In this scenario we wanted to understand how well these Other classes were detected when grouped together, and how the performance of the 6-Transient Classification Task was altered with the addition this new class.
    \item \textbf{7-Class Classification}: Recognize objects as belonging to one of the more common transient types in the dataset or as a Non-Transient object. The classes used were: AGN, Blazar, CV, Flare, HPM, Non-Transient and Supernovae. This task was developed with the purpose of understanding if classifiers could classify correctly the main transient types when the non-transient events were present.
    \item \textbf{8-Class Classification}: Recognize objects as belonging to any of the the following classes: AGN, Blazar, CV, Flare, HPM, Other, Non-Transient and Supernovae. The Other class was created by using objects from ambiguous and under-represented classes. This task was done to search how well was classification executed when testing together all the main classes that were present in the previous tasks.
\end{enumerate}

Three different algorithms were used for classification: neural networks, random forests and support vector machines. These algorithms were chosen because they were found to be popular in existing studies. Moreover, these three algorithms can perform quickly classification under the low dimensional feature data used, which means that their use in production pipelines may be studied too. Details on the inner workings of these machine learning models can be found in \cite{9780387848570}

To train machine learning models, it is required to define the training and testing processes. In our project, we made sure that the training data did not contain oversampled light curves based on any other present in the test samples, since this could suppose a bias. We also made sure that the test samples did not contain oversampled light curves, for the same reason. Additionally, since the number of objects in the class sub-sets was small we used 2-fold cross validation during training for result validation. Moreover, grid search was used during training to test multiple hyper-parameter configurations for each one of the possible algorithms. The evaluation metrics used to assess the performance of the model was the F1-Score. Finally, a test set was initially set aside for each task, and final scores were based on the prediction performance of the trained algorithms with these data subsets.


\subsection{Feature Importance} \label{subsection_importances}

Once the tasks were run and the best classifiers were obtained, we generated a list of the most relevant features. This was done using the best Random Forest classifier for each task. The goal behind this proposal was to understand which features contributed the most to the classification task. Doing this is critical in a machine learning context, since finding optimal feature sets enable lower computation times by sacrificing a little performance.
