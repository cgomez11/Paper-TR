The methodology applied in this work consists of six stages: filtering out irrelevant light curves, oversampling light curves, extracting descriptive features from the curves, processing them into feature vectors, re-scaling the resulting feature vectors, and performing classification using machine learning algorithms. The details of each step are detailed below.

\subsection{Data Filtering}

Astronomical object light curves with few observations may not contain enough information to be classified correctly by machine learning algorithms. 
This is why we decided to test the behaviour of our classification algorithms with light curves which contained higher averages of observations more. Precisely, an additional sub-set of light curves was generated by filtering out those which had less than 10 observations.


\subsection{Oversampling Transient Light Curves}

The dataset's number of transient event light curves per class was unbalanced. Thus, an oversampling step was implemented to test the behaviour of the classification algorithms when using the same amount of elements for each class.

In this project, oversampling means artificially generating multiple light curves, each based on a 'real' one. New curves were obtained by going through each observation of a given light curve, and generating a similar data-points independently for each synthetic curve. Every artificial observation created for a new light curve was sampled from a Gaussian distribution function centered on the base observation's apparent magnitude, with the magnitude's error used as standard deviation.

\subsection{Feature Extraction}

In general, astronomical observations are spread through time at irregular intervals, and their corresponding light-curves do not necessarily have the same number of observations. This makes it very challenging to directly use the time-series data for classification, with traditional methods. To solve this difficulty, a set of characteristic features were extracted from each light curve, using statistical and model-specific fitting techniques. Some measurements used in this project are formally introduced in \cite{1101.1959}, though these are used in many other studies, including \cite{1603.00882} and \cite{1601.03931}.

In total, 31 such features were used in this project. These features were computed for all light curves, and can be classified according to their origin in four groups: moment-based, magnitude-based, percentile-based and fitting-based. The groups are presented next.

\begin{enumerate}
    
    \item Moment-based features use the magnitude for each light curve.
    \begin{itemize}
        \item \underline{Beyond1std:} Percentage of observations which are over or under one standard deviation from the weighted average. Each weight used to compute the average is calculated as the inverse of the corresponding photometric error.
        \item \underline{Kurtosis}: The fourth moment of the data distribution. Used to measure the heaviness or lightness in the tails of the statistical data.
        \item \underline{Skewness:} A measurement of the level of asymmetry from the normal distribution in a data distribution. Negative skewness is the property of a more pronounced left tail, while positive skewness is a characteristic that implies a more pronounced right tail.
        \item \underline{Small Kurtosis:} Small sample kurtosis.
        \item \underline{Standard deviation:} The standard deviation of the magnitudes..
        \item \underline{Stetson J:} The Welch-Stetson J variability index \cite{1996PASP..108..851S}. A robust standard deviation.
        \item \underline{Stetson K:} The Welch-Stetson K variability index \cite{1996PASP..108..851S}. A robust kurtosis measure.
    \end{itemize}
    
    \item Magnitude-based use the magnitude for each source.
    \begin{itemize}
        \item \underline{Amplitude:} The difference between the maximum and minimum magnitudes.
        \item \underline{Max Slope:} Maximum absolute slope between two consecutive observations.
        \item \underline{Median Absolute Deviation:} The median of the difference between magnitudes and the median magnitude.
        \item \underline{Median Buffer Range Percentage:} The percentage of points within 10\% of the median magnitude.
        \item \underline{Pair Slope Trend:} Percentage of all pairs of consecutive magnitude measurements that have positive slope.
        \item \underline{Pair Slope Trend 30:} Percentage of the last 30 pairs of consecutive magnitudes that have a positive slope, minus percentage of the last 30 pairs of consecutive magnitudes with a negative slope.
    \end{itemize}
    
    \item Percentile-based features use the sorted flux distribution for each source. Flux is calculated as $F = 10^{0.4mag}$. Defining $F_{a,b}$ as the difference between the $b$ and $a$ flux percentiles.
    \begin{itemize}
        \item \underline{Percent Amplitude:} Largest percentage difference between the absolute maximum magnitude and the median.
        \item \underline{Percent Difference Flux Percentile:} Ratio of $F_{5,95}$ and the median flux.
        \item \underline{Flux Percentile Ratio Mid20:} Ratio $F_{40,60} / F_{5,95}$
        \item \underline{Flux Percentile Ratio Mid35:} Ratio $F_{32.5,67.5} / F_{5,95}$
        \item \underline{Flux Percentile Ratio Mid50:} Ratio $F_{25,75} / F_{5,95}$
        \item \underline{Flux Percentile Ratio Mid65:} Ratio $F_{17.5,82.5} / F_{5,95}$
        \item \underline{Flux Percentile Ratio Mid80:} Ratio $F_{10,90} / F_{5,95}$
    \end{itemize}
    
    \item Polynomial Fitting-based features are the coefficients of multi-level terms in high-level curve fitting. They use the magnitude for each source.
    \begin{itemize}
        \item \underline{Poly1 T1:} Linear term coeff. in monomial curve fitting.
        \item \underline{Poly2 T1:} Linear term coeff. in quadratic curve fitting.
        \item \underline{Poly2 T2:} Quadratic term coeff. in quadratic curve fitting.
        \item \underline{Poly3 T1:} Linear term coeff. in cubic curve fitting.
        \item \underline{Poly3 T2:} Quadratic term coeff. in cubic curve fitting.
        \item \underline{Poly3 T3:} Cubic term coeff. in cubic curve fitting.
        \item \underline{Poly4 T1:} Linear term coeff. in quartic curve fitting.
        \item \underline{Poly4 T2:} Quadratic term coeff. in quartic curve fitting.
        \item \underline{Poly4 T3:} Cubic term coeff. in quartic curve fitting.
        \item \underline{Poly4 T4:} Quartic term coeff. in quartic curve fitting.
    \end{itemize}

\end{enumerate}

The computed features were grouped in the following sets for experimentation:

\begin{enumerate}
    \item \underline{20 feats:} This sub-set includes all moment-based, magnitude-based and percentile-based features.
    \item \underline{26 feats:} This sub-set includes all features, except quartic curve fitting parameters (poly4).
    \item \underline{30 feats:} This sub-set includes all moment-based, magnitude-based, percentile-based and fitting-based features.
\end{enumerate}


\subsection{Feature Scaling}
Two feature scaling procedures were used in this project, making each feature weight equally when training machine learning algorithms. This process was applied independently to every single light curve feature. The first re-scaling procedure, named standardization, recalculates the values such that their updated mean is zero and they have unit variance. The second re-scaling feature used was normalization, which transforms features by re-scaling them to the range $[0,1]$. Note that these techniques were used exclusively in distinct experiments as explained in the experimentation section.

\subsection{Classification}

Five classification tasks were performed with the homogeneous computed feature vectors.

\begin{enumerate}
    \item \textbf{Binary Classification}: 
    Distinguish transients from non-transients. A balanced number of events from both classes is used to investigate the capability of distinguishing between these different types.
    \item \textbf{6-Transient Classification}: Recognize objects as belonging to one of the following transient types: AGN, Blazar, CV, Flare, HPM and Supernovae. The purpose of this task was to test how well did classification algorithms perform when distinguishing the main event types in the dataset.
    \item \textbf{7-Transient Classification}: Recognize objects as belonging to one of the following transient types: AGN, Blazar, CV, Flare, HPM, Other and Supernovae. The Other class was created by using objects from ambiguous and under-represented classes. In this scenario we wanted to understand how well these under-represented and ambiguous classes were detected, and how the performance of the 6-Transient Classification Task was altered.
    \item \textbf{7-Class Classification}: Recognize objects as belonging to one of the following classes: AGN, Blazar, CV, Flare, HPM, Non-Transient and Supernovae. This task was developed with the purpose of understanding if classifiers could classify correctly the main transient types when the non-transient events were present.
    \item \textbf{8-Class Classification}: Recognize objects as belonging to one of the following classes: AGN, Blazar, CV, Flare, HPM, Other, Non-Transient and Supernovae. The Other class was created by using objects from ambiguous and under-represented classes, just like in the 7-Transient Classification Task. This task was done to search how well was classification executed when testing together all the main classes that were present in the previous tasks.
\end{enumerate}

Three different algorithms were used for classification: neural networks, random forests and support vector machines. These algorithms were chosen because they were found to be popular in existing studies. Moreover, these three algorithms can perform quickly classification under the low dimensional feature data used, which means that their use in production pipelines could be studied too. Details on the inner workings of these machine learning models can be found in \cite{9780387848570}

Training machine learning models also requires defining the training and testing process. 2-fold cross cross validation used to validate the results, since the number of objects in the data sub-sets was small. Additionally, grid search was used to test multiple hyper-parameter configurations for each one of the algorithms. The evaluation metrics used to assess the performance of the model was the F1-Score. Keep in mind that a test set was initially set aside for each task, and final scores were based on this data subset.