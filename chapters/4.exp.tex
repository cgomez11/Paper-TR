All classification tasks described earlier were tested with multiple parameter variations. Every single task was run several times with different criteria (dataset inputs, pre-processing steps and algorithms), for the sake of investigating what configuration worked the best for each.

Five different parameters were used in these experiments, each with at least two possible values. All combinations of parameter variations were tried in all five classification tasks. The parameters used for experimentation are shown next.

\begin{description}
    \item \textbf{Minimum Observations} We used this parameter to test how transient classification varied using light curves with different amounts of observations. Specifically, two possible values were available for this setting: using light curves that contained a minimum of 5 observations each, or filtering out those with less than 10 observations minimum, and using the remaining curves only (Section \ref{subsection_filtering}).
    \item \textbf{Balancing Classes} Due to their inner workings, classification algorithms performance may differ if the number of elements for each class is not equal during training. This is why we experimented all tasks in two independent scenarios. Either we used the original CRTS light curves, which had unbalanced transient classes (Table \ref{Top-Transient-Classes}), or a sub-set of light curves with balanced classes. Such balanced dataset was made possible by taking into account artificially generated light curves (Section \ref{subsection_oversampling}). 
    
    To obtain balanced classes, those with a smaller count than the biggest class were incremented with synthetic light curves.
    
    Note that the Binary Classification task described in Section \ref{section_method} had by definition balanced classes only.
    
    \item \textbf{Number of Features} In order to test how much our classification results varied when using different feature subsets, three different subsets were tested for each task: 20, 26 or 30 features as defined in Section \ref{subsection_extraction}.
    \item \textbf{Feature Scaling} As described in Section \ref{subsection_scaling}, we tested independently scaling the features using either min-max scaling or standardization, since different feature scaling methods may result in distinct results.
    \item \textbf{Model Used} Three substantially different algorithms were tested, in order to investigate which learned the better from the data. Namely, they were Neural Networks, Random Forests or Support Vector Machines (Section \ref{subsection_classification}).
\end{description}
