All classification tasks described earlier were tested with multiple parameter variations. Every single task was run several times with different criteria (dataset inputs, pre-processing steps and algorithms), for the sake of investigating what configuration worked the best for each.

Five different parameters were used in these experiments, each with at least two possible values. All combinations of parameter variations were tried in all five classification tasks. The parameters used for experimentation are shown next.

\begin{description}
    \item \textbf{Minimum Observations} The objective of using this parameter was to test how transient classification varied using light curves with different amounts of observations. Specifically, two possible values were available for this setting: using light curves that contained a minimum of 5 observations each, or using light curves with minimum 10 observations each.
    \item \textbf{Balancing Classes} Due to their inner workings, classification algorithms performance may differ if during training the number of elements for each class is different. This is why we experimented all tasks in two independent scenarios. Either we used the original CRTS light curves (which had unbalanced transient classes as shown in Section \ref{section_data}) only, or a sub-set of light curves with balanced classes made possible by taking into account artificially generated light curves. To obtain balanced classes, the size of all classes smaller than the biggest class' were incremented with synthetic light curves. Note that oversampled classes weren't used for testing purposes (only training and validation), since this would bias our results. Also, recall that the Binary Classification task described in Section \ref{section_method} was defined as task with balanced data, which is why this doesn't apply in that scenario.
    \item \textbf{Number of Features} We wanted to test how much our classification results varied when using different subsets of the computed features. Three different subsets were tested for each task, either containing 20, 26 or 30 features as defined in Section \ref{section_method}.
    \item \textbf{Feature Scaling} Multiple feature scaling methods exist, some of which may result in better classification performance. As described in Section \ref{section_method}, we tested independently scaling the features using min-max scaling, or using standardization.
    \item \textbf{Model Used} Three substantially different algorithms were tested, in order to investigate which learned the better from the data. Namely, they were Neural Networks, Random Forests or Support Vector Machines.
\end{description}
