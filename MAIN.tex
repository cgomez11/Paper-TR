% mnras_template.tex
%
% LaTeX template for creating an MNRAS paper
%
% v3.0 released 14 May 2015
% (version numbers match those of mnras.cls)
%
% Copyright (C) Royal Astronomical Society 2015
% Authors:
% Keith T. Smith (Royal Astronomical Society)

% Change log
%
% v3.0 May 2015
%    Renamed to match the new package name
%    Version number matches mnras.cls
%    A few minor tweaks to wording
% v1.0 September 2013
%    Beta testing only - never publicly released
%    First version: a simple (ish) template for creating an MNRAS paper

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Basic setup. Most papers should leave these options alone.
\documentclass[a4paper,fleqn,usenatbib]{mnras}

% MNRAS is set in Times font. If you don't have this installed (most LaTeX
% installations will be fine) or prefer the old Computer Modern fonts, comment
% out the following line
%\usepackage{newtxtext,newtxmath}
\usepackage{import}
\usepackage{hyperref}

% Depending on your LaTeX fonts installation, you might get better results with one of these:
%\usepackage{mathptmx}
%\usepackage{txfonts}

% Use vector fonts, so it zooms properly in on-screen viewing software
% Don't change these lines unless you know what you are doing
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}



%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

% Only include extra packages if you really need them. Common packages are:
\usepackage{graphicx}	% Including figure files
\usepackage{amsmath}	% Advanced maths commands
\usepackage{amssymb}	% Extra maths symbols

% \usepackage{multicol}        % Multi-column entries in tables
% \usepackage{bm}		% Bold maths symbols, including upright Greek
\usepackage{pdflscape}	% Landscape pages
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{float} % here for H placement parameter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% AUTHORS - PLACE YOUR OWN COMMANDS HERE %%%%%

% Please keep new commands to a minimum, and use \newcommand not \def to avoid
% overwriting existing commands. Example:
%\newcommand{\pcm}{\,cm$^{-2}$}	% per cm-squared

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% TITLE PAGE %%%%%%%%%%%%%%%%%%%

% Title of the paper, and the short title which is used in the headers.
% Keep the title short and informative.
\title[Comparing machine learning algorithms for astronomical transient event recognition]{Comparing machine learning algorithms for astronomical transient event recognition}

% The list of authors, and the short list which is used in the headers.
% If you need two or more lines of authors, add an extra line using \newauthor
\author[Diego. A. Gomez et al.]{
Diego A. Gomez,$^{1}$\thanks{E-mail: da.gomez11@uniandes.edu.co}
Marcela Hernandez$^{1}$   
Jaime Forero-Romero$^{2}$
and Pablo Arbelaez$^{3}$
\\
% List of institutions
$^{1}$Departamento de Ingenier\'ia de Sistemas y Computaci\'on, Universidad de los Andes, Cra. 1 No. 18A-10, Bogot\'a, Colombia\\
$^{2}$Departamento de F\'isica, Universidad de los Andes, Cra. 1 No. 18A-10, Bogot\'a, Colombia\\
$^{3}$Departamento de Ingenier\'ia Biom\'edica, Universidad de los Andes, Cra. 1 No. 18A-10, Bogot\'a, Colombia
}

% These dates will be filled out by the publisher
\date{Accepted XXX. Received YYY; in original form ZZZ}

% Enter the current year, for the copyright statements etc.
\pubyear{2018}

% Don't change these lines
\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

% Abstract of the paper
\begin{abstract}

%\import{./chapters/}{0.abs.tex}
% This is a simple template for authors to write new MNRAS papers.
% The abstract should briefly describe the aims, methods, and main results of the paper.
% It should be a single paragraph not more than 250 words (200 words for Letters).
% No references should appear in the abstract.



The arrival of massive multi-epoch and multi-band astronomical surveys
demands the development of computational techniques to automate the
study and detection of transient astronomical sources. 
This paper describes an evaluation framework that allows for a standardized quantitative comparison of astronomical transient event recognition algorithms. 
We do this by characterizing various machine learning algorithms to
recognize and classify astronomical transient events.
We use light-curves from the Catalina Real Time Transient Survey
(CRTS) to classify thousands of unique transient and non-transient
event light-curves.
We present this annotated dataset, which contains more than $10.000$ transient and non-transient object light curves.
We experiment with multiple data pre-processing methods,
feature selection techniques and self-learning models. 
We obtain new results in five classification tasks, paying particular
attention to a binary and a four multi-class classifications task.  
We conclude that the best performing algorithm was
a Random Forest with an f1-score of 87.27\% on binary
(transient \& non-transient) classification. 
Six-class transient classification achieved a 77.54\% f1-score, and a 
66.39\% f1-score when including an ambiguous sources class.
Multi-class transient classifications including non-transient sources,
resulted in a 75.05\% f1-score and a 66.05\% f1-score respectively.
We provide the curated datasets, code and trained models used in this project.

\end{abstract}

% Select between one and six entries from the list of approved keywords.
% Don't make up new ones.
\begin{keywords}
methods: data analysis, statistical
%keyword1 -- keyword2 -- keyword3
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% BODY OF PAPER %%%%%%%%%%%%%%%%%%

\section{Introduction}
%\import{./chapters/}{1.intro.tex}
% All papers should start with an Introduction section, which sets the work
% in context, cites relevant earlier studies in the field by \citet{Others2013},
% and describes the problem the authors aim to solve \citep[e.g.][]{Author2012}.

%%% NEW MULTI EPOCH SURVEYS ENABLE STUDY AND DETECTION IN CRAZY SCALES
The study and detection of astronomical variable sources is expected
to occur on unprecedented scales with the new generation of
forthcoming multi-epoch and multi-band (synoptic) astronomical
surveys. 
For instance, the Large Synoptic Survey Telescope (LSST)
\citep{0805.2366}, one of the largest synoptic survey telescopes to
come in the following years, will generate approximately 15 terabytes
of data every night \citep{1512.07914}.  
%Such telescope is expected to detect and alert about 10 million possible transients too. 
Other surveys including the Square Kilometer Array (SKA) are also expected
to generate exuberant daily data-streams. 

%%% HISTORICALLY IT HAS BEEN DONE MANUALLY, NOW IT ISN"T POSSIBLE
This observational leap renders manual classification techniques
unfeasible.   
Traditionally, such objects have been classified by visual inspection
by experts or through crowd-sourcing
\citep{1011.2199,0708.2750}. 
This approach is slow and expensive.
Another concern is the possible biases and difficulty to standardize among 
astronomers \citep{1104.3142}. 
Alternatively, transient detection can be executed much faster than
human astronomers through computational techniques using machine
learning, 
which are deterministic and calculate the results' degrees of
certainty. 
These methods also allow for real-time triggering of follow-up
observations that optimize the economical and temporal resources. 
 

%%% TRANSIENTS EVENTS ARE CHALLENGES (AND WHAT TRANSIENT EVENTS ARE)
Another challenge in  Time Domain Astronomy is Real-Time Transient classification. 
Astronomical Transients are events whose luminosity varies in short duration
relative in the timescale of the universe, from minutes to several
years. 
Transients include phenomena such as supernovae, novae, neutron
stars, blazars, pulsars, cataclysmic variable stars (CV), gamma ray
bursts (GRB) and active galaxy nuclei (AGN). 
The time-domain dependency of these objects is one of the reasons why
they are hard to classify: their data is usually heterogeneous,
unbalanced, sparse, unevenly sampled and with missing information. 
Automating the recognition and classification of transient events, a type of such
variable sources, would reduce costs and speed up this process as well
as provide scientists information of the universe in various spatial
scales \citep{2011arXiv1110.4655D}.  

%%% HOW DOES TRANSIENT DETECTION
Transient detection is generally done through 
difference imaging \citep{1507.05137,1608.01733,1708.02850}. 
This algorithm starts by aligning the image of interest on a reference image of the
same region of sky, processing the former to match its point-spread
function in all regions with the latter's, and subtracting both
\citep{astro-ph/9712287}. 
As a result, variable stars and transient objects which were not
visible in the reference image will remain in the resulting
image. 
Nonetheless, the difference images can also contain
artifacts due to imperfections in the image processing
phase, in the telescope or other phenomena. 
Unfortunately, distinguishing between bogus and real transient objects
still requires human intervention, making it expensive and slow
\citep{2011arXiv1110.4655D}.  


%%% STATE OF THE ART I: REAL VS BOGUS
There have been successful attempts to implement automatic detection
algorithms and distinguish artifacts from real transient
objects.  
For instance, raw images from the  SkyMapper Supernova and Transient
Survey and the High cadence Transient Survey (HiTS) have been used as
inputs to automatic detection algorithms \citep{1708.08947,1701.00458}.
Convolutional Neural Networks (CNN) have also achieved
high accuracy in this binary classification task.
Other studies have shown that artifacts can be detected using
features extracted from raw images. 
\cite{1601.06151} and \cite{1601.06320}, achieved reliable
classification by transforming transient data from the OGLE-IV
data-reduction pipeline and training it with machine learning
algorithms such as Artificial Neural Networks, Support Vector
Machines, Random Forests, Naive Bayes, K-Nearest Neighbors and Linear
Discriminant Analysis.  
Similarly, \cite{1501.05470} used images from Pan-STARS1 Medium Deep
Surveys, and \cite{1407.4118} processed single-epoch multi-band images
from the SDSS supernova survey for the same purpose.  


%%% STATE OF THE ART II: DIRECT

Some alternative approaches classify astronomical transient
events without image subtraction.
This is usually done by extracting relevant features (e.g. periodic,
non-periodic) from the astronomical object light-curves, and using
machine learning algorithms for classification. 
For instance, in \cite{1401.3211} and \cite{1601.03931}, light curves
of objects six or more transient classes from the Catalina Real Time
Transient Survey and the Downes set (\cite{d05}) were
classified using this approach.
\cite{1603.00882} used the same approach to find
where supernovas from the Supernova Photometric Classification
Challenge.
Recurrent Neural Networks (RNNs) have also been proven successful to
classify transient events, \cite{1606.07442} and \cite{1710.06804}
showed that  modern algorithms can also learn from time-series data
without expensive image processing.    

% PROPOSAL
This project is a compiled study that unifies techniques used in previous studies, in a structured way. We openly provide the curated datasets and the code used in this project, so that others may replicate the results and expand upon them (Section \ref{section_code}). Thus, with this paper we provide an experimental framework that can be used as a baseline for future research.

In this paper we follow the mentioned approach and test 
different machine
learning algorithms to classify transient events using light curves as an input.  
We start by extracting several statistical descriptors from the light
curves.
These descriptors are the input of three different machine learning
models with several hyper-parameter variations: Support Vector
Machines (SVM), Neural Networks (NN) and Random Forests (RF), which automatically
learn to detect and classify between different types of transient
objects. 
We present the results of extensive testing and multiple experiments
executed in order to find the best training parameters. 



% STRUCTURE
The paper is structured as it follows. In Section \ref{section_data}
we present the dataset used for this project. 
Section \ref{section_method} describes the methodology.
Section \ref{section_experimentation} explains the experiments
performed to classify transient objects with machine
learning. 
Finally, the results are presented and discussed in
Section \ref{section_results}.  

\section{Data} \label{section_data}
%\import{./chapters/}{2.data.tex}
%%% Overview

We use public data from the Catalina Real-Time Transient Survey
(CRTS) \citep{1111.2566}, an astronomical survey searching transient
and highly variable objects.   
It covered 33000 squared degrees of sky and took data since 2007.
Three telescopes were used: Mt. Lemmon Survey (MLS), Catalina Sky
Survey (CSS), and Siding Spring Survey (SSS). So far, CRTS has
discovered more than $15000$ transient events.
We use light curves as measured with the CSS telescope of the CRTS, which is
an f/1.8 Schmidt telescope located in the Santa Catalina Mountains, north of Tucson,
Arizona and is equipped with a 111-megapixel  detector, and covered
4000 square degrees per night, with a limiting magnitude of 19.5 in
the V band.  
The public CRTS data base reports the source flux and its
corresponding uncertainty \citep{1996PASP..108..851S}.


All transient objects were classified in the CRTS dataset according to
their type. 
The most relevant classes are: supernovae (SN),
cataclysmic variable stars (CV), blazars, flares, asteroids, active
galactic nuclei (AGN), and high-proper-motion stars (HPM). 
Though most objects in the transient object catalogue belong to a single class,
there is some uncertainty in the categorization of some of
them. 
In this case, an interrogation sign is used when a class is not clear
e.g. SN? or sometimes multiple possible classes are found for a single
event e.g. SN/CV.
Table \ref{Top-Transient-Classes} summarized The number of objects in each class.


We use the light curves of $4269$ unique transient events that
contain at least 5 observations.
We also use $15193$ non-transient sources with at least 5 observations each. 
We obtained sources from the transient dataset directly from researchers of the CRTS project. Alternatively, we retrieved sources in the dataset from the
CRTS online catalogue, by sampling light curves of objects within a 0.006 
degree radius from CRTS detected transients, and removing known transient 
light curves from that set. 
Though this process should return only non-transient sources, it is
possible that non-detected transients were captured and catalogued
incorrectly as 'non-transients'.  
Table \ref{Transient-Observation-Count} and
Table \ref{Non-Transient-Observation-Count} summarize some statistics
on the number of observations available for each light curve for the
transient and non-transient datasets, respectively.  



\import{./figures/ObjCount/}{5.tex}


\section{Methodology} \label{section_method}
%\import{./chapters/}{3.method.tex}

Our methodology has six stages: 
filtering out irrelevant light curves, oversampling light curves, extracting
descriptive features from the curves, processing them into feature
vectors, re-scaling those resulting feature vectors, and 
classifying them with machine learning algorithms. 
The details of each step are in the following subsections.

\subsection{Data Filtering} \label{subsection_filtering}

We discard light curves with few observations as they may not contain
enough information to be classified correctly by machine learning
algorithms.  
The nominal cut is 5 observations per light curve, but we also test a
higher cut by filtering out light curves with less than 10 observations.

\subsection{Oversampling Transient Light Curves} \label{subsection_oversampling}

The number light curves per class is unbalanced. 
In order to have the same amount elements for each class we implement an
oversampling step by artificially generating multiple mock light curves,
each based on an observed one. 

We generate a mock light curve from the observed light curve and 
then sample the observed magnitude from a Gaussian distribution
centered on the observational apparent magnitude with the magnitude's
error as the  standard deviation. 


\subsection{Feature Extraction} \label{subsection_extraction}

Light curves are sampled at irregular time intervals and have
different numbers of data points.
Thus, it is challenging to directly use the time-series data for
classification with traditional methods.
We circumvent this problem by extracting a set of features for each
light curve.
These features are scalars derive from statistical and model-specific
fitting techniques.
Some of these features were formally introduced in
\cite{1101.1959}, and have been used in other studies
\citep{1603.00882,1601.03931}.  

We use 30 features in total that can be classified into four groups:
moment-based, magnitude-based, percentile-based and fitting-based. 
In this project we propose the fitting-based features.
These groups are:

\begin{enumerate}
    
\item Moment-based features, which use the magnitude for each light curve.
  \begin{itemize}
  \item \underline{Beyond1std} (\textit{beyond1std}): 
    Percentage of observations which are over or under one standard
    deviation from the weighted average. Each weight is calculated as
    the inverse of the corresponding observation's photometric error. 
        \item \underline{Kurtosis} (\textit{kurtosis}): 
        The fourth moment of the data distribution. Used to measure
        the heaviness or lightness in the tails of the statistical
        data. 
        \item \underline{Skewness} (\textit{skew}): 
        A measurement of the level of asymmetry from the normal
        distribution in a data distribution. Negative skewness is the
        property of a more pronounced left tail, while positive
        skewness is a characteristic that implies a more pronounced
        right tail. 
        \item \underline{Small Kurtosis} (\textit{sk}):
        Small sample kurtosis.
        \item \underline{Standard deviation} (\textit{std}):
        The standard deviation of the magnitudes..
        \item \underline{Stetson J} (\textit{stetson\textunderscore j}):
        The Welch-Stetson J variability index
        \cite{1996PASP..108..851S}. A robust standard deviation. 
        \item \underline{Stetson K} (\textit{stetson\textunderscore k}): 
        The Welch-Stetson K variability index
        \cite{1996PASP..108..851S}. A robust kurtosis measure. 
    \end{itemize}
    
    \item Magnitude-based features, which relies on the magnitude for each source.
    \begin{itemize}
    \item \underline{Amplitude} (\textit{amp}): 
      The difference between the maximum and minimum magnitudes.
    \item \underline{Max Slope} (\textit{max\textunderscore slope}): 
      Maximum absolute slope between two consecutive observations.
    \item \underline{Median Absolute Deviation} (\textit{mad}): 
      The median of the difference between magnitudes and the median
      magnitude. 
    \item \underline{Median Buffer Range Percentage} (\textit{mbrp}): 
      The percentage of points within 10\% of the median magnitude.
    \item \underline{Pair Slope Trend} (\textit{pst}): 
      Percentage of all pairs of consecutive magnitude measurements that have positive slope.
    \item \underline{Pair Slope Trend 30} (\textit{pst\textunderscore last30}): 
      Percentage of the last 30 pairs of consecutive magnitudes that have a positive slope, minus percentage of the last 30 pairs of consecutive magnitudes with a negative slope.
    \end{itemize}


  \item Percentile-based features use the sorted flux distribution for
    each source. The flux is computed as $F = 10^{0.4 \mathrm{mag}}$. 
    We define $F_{n,m}$ as the difference between the $m$-th and $n$-the flux
    percentiles. 
    \begin{itemize}
    \item \underline{Percent Amplitude} (\textit{p \textunderscore amp}): 
      Largest percentage difference between the absolute maximum magnitude and the median.
    \item \underline{Percent Difference Flux Percentile} (\textit{pdfp}): 
      Ratio between $F_{5,95}$ and the median flux.
    \item \underline{Flux Percentile Ratio Mid20} (\textit{fpr20}): 
      Ratio $F_{40,60} / F_{5,95}$
    \item \underline{Flux Percentile Ratio Mid35} (\textit{fpr35}):
        Ratio $F_{32.5,67.5} / F_{5,95}$
      \item \underline{Flux Percentile Ratio Mid50} (\textit{fpr50}): 
        Ratio $F_{25,75} / F_{5,95}$
      \item \underline{Flux Percentile Ratio Mid65} (\textit{fpr65}): 
        Ratio $F_{17.5,82.5} / F_{5,95}$
      \item \underline{Flux Percentile Ratio Mid80} (\textit{fpr80}): 
        Ratio $F_{10,90} / F_{5,95}$
    \end{itemize}
    
  \item Polynomial Fitting-based features are the coefficients of
    multi-level terms in polynomial curve fitting. This is new set
    of features proposed in this paper. 
    \begin{itemize}
        \item \underline{Poly1 T1}: Linear term coeff. in monomial curve fitting.
        \item \underline{Poly2 T1}: Linear term coeff. in quadratic curve fitting.
        \item \underline{Poly2 T2}: Quadratic term coeff. in quadratic curve fitting.
        \item \underline{Poly3 T1}: Linear term coeff. in cubic curve fitting.
        \item \underline{Poly3 T2}: Quadratic term coeff. in cubic curve fitting.
        \item \underline{Poly3 T3}: Cubic term coeff. in cubic curve fitting.
        \item \underline{Poly4 T1}: Linear term coeff. in quartic curve fitting.
        \item \underline{Poly4 T2}: Quadratic term coeff. in quartic curve fitting.
        \item \underline{Poly4 T3}: Cubic term coeff. in quartic curve fitting.
        \item \underline{Poly4 T4}: Quartic term coeff. in quartic curve fitting.
    \end{itemize}    
\end{enumerate}

We group these features into three sets for experimentation:

\begin{enumerate}
\item \underline{20 feats:} Includes all moment-based, magnitude-based
  and percentile-based features.  
\item \underline{26 feats:} Includes all features, except quartic
  curve fitting parameters (poly4). 
\item \underline{30 feats:} Includes all features.
\end{enumerate}


\subsection{Feature Scaling} \label{subsection_scaling}

We use two feature scaling procedures to weight equally each input
for the machine learning training process.
These procedures are applied to the magnitudes for each light curve.
The first one is standardization, where the re-scaled values have zero mean 
and unit variance. 
The second one is normalization, were the magnitudes are re-scaled  to the
range $[0,1]$. 
These procedures are part of the modules that are switched on/off as
part of the experimentation explained in Section \ref{section_experimentation}.


\subsection{Classification} \label{subsection_classification}

We study five classification tasks of varying complexity.

\begin{enumerate}
    \item \textbf{Binary Classification}: 
      We use a balanced number of events from both classes is used 
      to investigate the capability of distinguishing between Transients
      and Non-Transients.
    \item \textbf{6-Transient Classification}: To recognize objects as
      belonging to one of the more common transient types in the dataset,
      namely: AGN, Blazar, CV, Flare, HPM and Supernovae. The purpose of
      this task is to test the accuracy of classification algorithms
      when distinguishing only the main transient event types in the
      dataset. 
    \item \textbf{7-Transient Classification}: To recognize objects as
      belonging to one of the more common transient types in the dataset
      or as a different transient, namely: AGN, Blazar, CV, Flare, HPM,
      Other and Supernovae. The Other class was created by using objects
      from ambiguous and under-represented classes such as the ones
      described in Section \ref{section_data}. In this scenario we wanted
      to understand how well these Other classes are detected when
      grouped together, and how the performance of the 6-Transient
      Classification Task changes with the addition of this new class. 
    \item \textbf{7-Class Classification}: To recognize objects as
      belonging to one of the more common transient types in the
      dataset or as a Non-Transient object. We use the classes:
      AGN, Blazar, CV, Flare, HPM, Non-Transient and Supernovae. 
      The purpose of this experiment is understanding whether classifiers 
      could recognize correctly the main transient types when the 
      non-transient events are present.  
    \item \textbf{8-Class Classification}: To recognize objects as
      belonging to any of the the following classes: AGN, Blazar, CV,
      Flare, HPM, Other, Non-Transient and Supernovae. 
      The Other class is built using objects from ambiguous and
      under-represented classes. 
      The goal of this task is to assess the quality of classification when
      testing together all the main classes in the previous tasks.
\end{enumerate}

We conduct experiments with three widely used families of classification 
algorithms: Neural Networks, Random Forests and Support Vector Machines.
These algorithms are popular in published studies and are efficient 
for low dimensional feature datasets as is our case. 
We use SciKit Learn's \citep{1201.0490} Python's implementation of these algorithms.
Details on the inner workings of these machine learning models can be
found in \cite{9780387848570}.  

We test NNs with all the combinations of the following hyper-parameter values:
\begin{itemize}
    \item Learning Rate: Either constant vs adaptive.
    \item Hidden Layer Sizes: Single Layer with $100$ nodes vs Two layers with
    $100$ nodes each.
    \item L2 Penalty (\textit{alpha}): $1e-1$ vs $1e-2$ vs $1e-3$ vs $1e-4$.
    \item Activation Function: Logistic vs Relu.
\end{itemize}

Similarly, we test RFs with all the combinations of the following 
hyper-parameter values:
\begin{itemize}
    \item Number of Estimators: $200$ or $700$.
    \item Number of features Considered: Square Root or the Logarithm 
    base 2 of the total number of features.
\end{itemize}

We also test RFs with all the combinations of the following hyper-parameter values:
\begin{itemize}
    \item Kernel: Radial Basis Function (RBF).
    \item Kernel Coefficient (\textit{gamma}): 1e-1 vs 1e-2 vs 1e-3 vs 1e-4 vs 1e-5.
    \item Error Penalty (\textit{C}): 1 vs 10 vs 100 vs 1000. 
\end{itemize}

We also make sure that the training data does not contain oversampled
light curves based on any other present in the test samples, since
this could introduce a bias.  
Similarly, the test samples contain not contain oversampled light
curves.
Additionally, since the number of objects in the class sub-sets was
small, we used 2-fold cross-validation during training for as evaluation
protocol. 
Moreover, we used grid search during training to test multiple
hyper-parameter configurations for each one  of the possible
algorithms. 
We use the F1-Score to assess the performance of a given model.
We evaluate each task on the held-out test sets described above.


\subsection{Feature Importance} \label{subsection_importances}

After finding the best classifiers we generate a list of the most
relevant features. 
We do this with the best Random Forest classifier for each task.
We calculate feature importance using the Python library 
SciKit-Learn \citep{1201.0490}. We used the mean decrease impurity, 
which is calculated as the total decrease in node impurity averaged over all the trees belonging to
the ensemble.
We normalized the resulting scores, so that the sum of all features
importance is equal to 1.
This approach quantifies which features contributed the most to the
classification task.  

\section{Experiments} \label{section_experimentation}

%\import{./chapters/}{4.exp.tex}
We perform all classification tasks described earlier with multiple
parameter variations.  
We run multiple times each task with different criteria 
(dataset inputs, pre-processing steps and algorithms), 
to find out the best configuration for each case.

We vary five different hyper-parameters in our experiments.
Each experiment includes at least two possible values for each
hyper-parameter.
We try all possible combinations of all hyper-parameter variations 
for the five classification tasks. 
These five parameters are the following. 

\begin{enumerate}
    \item \textbf{Minimum number of Observations}. 
      This quantifies how  different amounts of observations
      per light curve impact transient classification.
      We use two values for this parameter: 5 and 10 (Section \ref{subsection_filtering}). 
    \item \textbf{Balancing Classes}. 
      We experiment with two different scenarios.
      First, we use the original CRTS light curves, with unbalanced transient classes (Table
      \ref{Top-Transient-Classes}).
      Second, we use a sub-set of light curves with balanced classes. 
      We balanced the classes by increasing the number of light 
      curves with synthetic ones, for classes with smaller count than the biggest class (Section
      \ref{subsection_oversampling}).
      Note that the Binary Classification task described in Section
      \ref{section_method} has by construction balanced classes only.
    
    \item \textbf{Number of Features}. 
      We use different feature subsets. 
      The three different subsets have 20, 26 or 30 features as
      defined in Section \ref{subsection_extraction}. 
    \item \textbf{Feature Scaling}. 
      We test two different feature scaling procedures: normalization
      scaling and standardization as defined in Section
      \ref{subsection_scaling}. 
    \item \textbf{Classifier}. 
      The three classification algorithms described in Section 
      \ref{subsection_classification}: Neural Networks, 
      Random Forests and Support Vector Machines 
\end{enumerate}


\section{Results} 
\label{section_results}

%%%%%%  BINARY  %%%%%%
\subsection{Binary Classification} 
\label{Results-Binary} 

\import{./figures/Binary/}{class_5.tex}
\import{./figures/Binary/}{class_10.tex}


% Best Clf.
The best algorithm in this task is RF with a maximum f1-score of
87.69\%.   
This score is achieved with light curves containing minimum 5
observations, and 30 features.
Using more than 20 features represents a significant increase in
performance.
Using 30 features rather than 26 is marginally better. 
Table \ref{Classifier-Scores-Binary-5} and Table 
\ref{Classifier-Scores-Binary-10} summarize all the results.


% SVM
SVMs are the second best-performing model with the f1-score of 85.36\%. 
Changing the number of features does not affect significantly the score.
%However, using light curves with more than 10 observations results in
%slightly better results. 

% NN
NN is ranked third, although its scores are very similar to SVMs. 
The highest achieved score for NN is 85.03\%.
%Scores for min\textunderscore obs = 5 are higher than the ones
%obtained with SVMs, but the opposite occurs when min\textunderscore
%obs = 10.  
We do not find a clear correlation between score values and other
parameters.  

% Confusion Matrix
Table \ref{Confusion-Binary} shows the confusion matrix of the best
performing algorithm and Table \ref{Overall-Scores-Binary} summarizes
the scores.
These results imply that non-transients are better classified overall.  


% Importances
\import{./figures/Binary/}{importances.tex}

Figure \ref{Importances-Binary} displays the most important features
for the RF classifier.
The top five inputs for classification are stetson\textunderscore j,
std, mad, poly1\textunderscore t1 and poly2\textunderscore t1.  
The former achieved the highest importance with over 21\%, compared to
the following with values in the range 6\% - 8\%. 
The least relevant features are the polynomial curve-fitting
coefficients that contributed less than 1\% each.


%%%%%%  SIX-TRANSIENT  %%%%%%
\subsection{Six-Transient Classification}

\import{./figures/6-Transient/}{class_5.tex}
\import{./figures/6-Transient/}{class_10.tex}

% Best Clf.
The best algorithm is RF with a top f1-score of 77.54\% achieved with 
unbalanced inputs, 30 features and light curves with a minimum of 10
observations. 
Table \ref{Classifier-Scores-6-Transient-5} and Table
\ref{Classifier-Scores-6-Transient-10} summarize all the
results. 
 
% RF
On average, RF is the best-performing model for this task. 
The performance improves with more than 20 features; using 30 features 
rather than 26 is marginally better for the unbalanced dataset. 
Using unbalanced light curves tend to work best.

% NN
NN ranks second in this task, scoring a maximum f1-score of 73.01\%. 
Using unbalanced inputs with this model improves performance.
Having more than 20 features show a moderate increase in performance. 
Using unbalanced light-curves correlates to a higher score with 10 
observations minimum, whereas in balanced inputs does
not show a clear trend.

% SVM
SVMs are last in this ranking with a maximum f1-score of 69.65\% and
scores very similar to those of NN. 
A significant increase in performance is obtained with this model when
using more than 20 features, though the difference between using 26
and 30 features is very small. 
On the other hand, there is a small score increase when training
on unbalanced inputs.

% Confusion Matrix
Table \ref{Confusion-6-Transient} shows the confusion matrix for RF. 
There, HPM is the best performing class with a recall of 94.32\%, 
followed by SN and AGN, with a recall of 86.67\% and 80.31\%
respectively. 
Blazar and Flare are the two classes with the lowest recall; 42.86\%
and 55.36\%, respectively. 
The confusion matrix shows that most of the incorrectly
classified AGN, Blazar, CV and Flare instances were identified as
SN. A big proportion of Blazar light curves were also incorrectly
classified as AGNs and CVs, though this amount is low compared to the
false SN samples. 


% Importances
\import{./figures/6-Transient/}{importances.tex} Figure
\ref{Importances-6-Transient} displays the feature importance rank in
descending order for the RF classifier.
Stetson\textunderscore j, std, amp, mad, and sk, have an importance above 6\%. 
The remaining features have similar importance values, except
for the highest level polynomial coefficients which rank last.
This list presents a slightly more balanced importance among features,
compared to the results in the Binary Classification Task in the
previous sub-section.

% Refer to Light-Curve Visualizations

%%%%%%  SEVEN-TRANSIENT  %%%%%%
\subsection{Seven-Transient Classification}

\import{./figures/7-Transient/}{class_5.tex}
\import{./figures/7-Transient/}{class_10.tex}

% Best Clf.
The best algorithm is RF with a top f1-Score of 66.39\% achieved with 
min\textunderscore obs = 10, 30 features and balanced inputs.
Table \ref{Classifier-Scores-7-Transient-5} and Table
\ref{Classifier-Scores-7-Transient-10} summarize the results.

% RF
RF outperform all other models in their respective parameter
combinations. 
We find a significant increase in performance when using more than 20
features. 
Balanced inputs yield a better overall performance except when using 20 
features. 
%Using light curves with minimum 10 observations, rather than 5
%observations, represents a slights improvement.

% NN
NN are second in this task, achieving a top score of 61.28\%.  
%NN achieve better results when using light curves that contained a
%minimum 10 observations. 
We do not find a clear correlation between performance and the number
of features used. 


% SVM
SVMs ranked third with a maximum f1-score of 58.62\%. 
These classifiers have a slightly worse scores than NN.
%The best results are obtained using light curves with a minimum of 10
%observations.
Moreover, we obtained higher average results when using balanced inputs,
except when using balanced inputs and min\textunderscore obs = 5.
Finally, the number of features does not strongly influence the final
result in most cases.

% Confusion Matrix
Table \ref{Confusion-7-Transient} shows the confusion matrix for the
best model. 
HPM is by a $\sim$20\% difference the class with the most correctly
classified instances.  
The worst performing classes are Blazar and Other, achieving a recall
close to 48.5\%. 
All other classes have a recall between 60\% and 80\%. 
The most incorrectly classification were AGNs identified as belonging
to the Other class. 
Additionally, most incorrectly classified light-curves belong to Blazar, CV, Flare and
Other classes are identified as SN. 
Comparing this Table against the  6-Transient Classification Task confusion matrix (Table
\ref{Confusion-6-Transient}) we conclude that the inclusion of the
Other class results in a performance deterioration for various classes
including AGN and SN.   

% Scores
% In the class scores table for this task, it is shown that both the averaged Precision and Recall were very similar (Table \ref{Overall-Scores-7-Transient-Oversampled}). It is clear too that precision-recall individual class values were very similar, resulting in a difference of at most 7\% for each class, except in the case of Flares where it is much higher.


% Importances
\import{./figures/7-Transient/}{importances.tex}

Figure \ref{Importances-7-Transient} displays the feature importance
ranking for the best classifier.
This raking follows a similar trend as Figure \ref{Importances-6-Transient}. 
The most relevant features for this task are stetson\textunderscore j,
sk, std, mad, and amp which achieved an importance of over 6\% each. 
The least important features are the highest level polynomial
coefficients.


%%%%%%  SEVEN-CLASS  %%%%%%
\subsection{Seven-Class Classification}

\import{./figures/7-Class/}{class_5.tex}
\import{./figures/7-Class/}{class_10.tex}

% Best Clf
The best algorithms is RF with an f1-score of 75.05\% achieved with 
curves containing at least 10 observations, using 30 features and
unbalanced inputs. 
Table \ref{Classifier-Scores-7-Class-5} and Table
\ref{Classifier-Scores-7-Class-10} summarize the results.

% RF
Overall, RF is the best model, always performing better with unbalanced
inputs.  
The worst results are obtained using 20 features only.
Finally, using light curves with more than 20 features resulted in
better scores.

% NN
NN is the second-best performing algorithm with a maximum f1-score of
69.14\%. 
%In general, the use of min\textunderscore obs = 10 generates higher
%scores. 
Moreover, training with a higher amount of features correlates with a
higher performance, except only when using min\textunderscore obs = 10
and unbalanced inputs, where the results are very similar. 
Finally, using unbalanced inputs gives significantly higher scores.  

% SVM
SVM is the the worst algorithm with a maximum f1-score of 65.85\%. 
%In general, higher scores are obtained from light curves with a
%minimum of 10 observations.  
Using balanced inputs also results in slightly better results. 
Finally, training with 26 features tends to give the best scores,
although those are very similar to the results with 31 features.  

% Confusion Matrix
Table \ref{Confusion-7-Class} shows the confusion matrix for the best
model. 
HPMs and Non-Transient samples have the most correctly classified elements, with 84.09\%
and 87.62\% recall respectively. 
Other classes such as AGN, CV and SN follow with a recall in the 74\%
- 78\% range. 
Blazars and Flares on the other hand have the lowest scores (below
40\%). 
SN is the class with which most other class instances incorrectly
classified, except for Flares, where 50\% of the test samples are
classified as Non-Transients. 
We conclude that the inclusion of the Non-Transient class slightly
reduces the  classification performance of transient classes in
comparison against the  6-Transient classification task
(Table \ref{Confusion-6-Transient}).  

% Scores
% In the class scores table for this task, it is shown that both the averaged Precision and Recall were very similar (Table \ref{Overall-Scores-6-Transient-Oversampled}). On the other hand, individual Blazar precision is found to be much higher than its initial recall discussed previously, just as in the 6-Transient task (Table \ref{Overall-Scores-7-Transient-Oversampled}). 
Notice that the inclusion of Non-Transients decreases flares f1-score,
implying a worse recognition of this class. All other classes
f1-scores were also decreased by little with the inclusion of
Non-Transients (Table \ref{Overall-Scores-7-Class-Regular}). 


% Importances
\import{./figures/7-Class/}{importances.tex}

Figure \ref{Importances-7-Class} displays the feature importance
ranking. 
The general trend is similar to 6-Transient classification in Figure
\ref{Importances-7-Class}. 
The highest importance features for this task are the
stetson\textunderscore j index with a $\sim$9\% importance, followed by
amp, std, sk, and mad, with an importance close to 6\%. 
Give of the highest level polynomial coefficients are the least
important: poly4\textunderscore t1, poly4\textunderscore t2,
poly3\textunderscore t3, poly4\textunderscore t3 and
poly4\textunderscore t4; note that poly4\textunderscore t1 scores a
much higher importance.  


% Refer to Light-Curve Visualizations



%%%%%%  EIGHT-CLASS  %%%%%%
\subsection{Eight-Class Classification}

\import{./figures/8-Class/}{class_5.tex}
\import{./figures/8-Class/}{class_10.tex}


% Best Clf.
RF is the best classifier. The best f1-score is 66.05\% achieved with
30 features, unbalanced inputs and light curves containing minimum 10
observations.  
Table \ref{Classifier-Scores-8-Class-5} and Table
\ref{Classifier-Scores-8-Class-10} summarize the results.

% RF
RF is the overall best performing model.
The algorithm's performance substantially increases when using 26 -
30 features, compared to 20 features.
There is a marginal improvement when using non-balanced inputs. 
%Using light-curves with min\textunderscore obs = 10 always results in
%higher scores.  

% NN
NN are the second best. 
Its highest f1-score was 60.19\%. 
%Increasing  the minimum number of observations per light curve from 5
%to 10 results in a considerable performance upgrade.  
Training on unbalanced inputs is another factor that increases the
scores. 
The number of features used for training is directly correlated with
the scores.
With unbalanced classes, the higher the number of features, the
better. 
Alternatively, when using balanced inputs, using 26 and 30 features
yields the highest scores. 
Nonetheless, using 26 features is marginally better. 

% SVM
SVM is the worst-performing model.
It only achieves a maximum f1-score of 57.30\%.
%Using light curves with 10 observations results in higher scores, except 
%when using 20 features with balanced inputs. 
Using balanced inputs works best with light curves that have 5 
observations minimum, whereas unbalanced inputs
resulted in better results with light curves that had 10 observations
minimum.  
Finally, the number of features used does not affect much the model's
performance.%, unless min \textunderscore obs is 10, in which 26 and 30
%features result in better scores. 

% Confusion Matrix
Table \ref{Confusion-8-Class} presents the confusion matrix.
The two classes with highest recalls are HPM and Non-Transient, with a
recall of 86.36\% and 84.13\%, respectively. 
The worst performing classes are Blazar, Flare and Other, with recall
values in the range 36\% - 40\%. 
SN was the class with which most other class instances are
incorrectly classified. 
Moreover, Flares had about 50\% of the test samples classified as
Non-Transients, AGNs had about 20\% of their 
samples classified as Other, and Blazars and Other had most of  its
samples classified as AGN. 
Additionally, most incorrectly classified AGNs ($\sim$20.5\%) are
identified as Other and most Blazar instances are
incorrectly categorized as either SN or AGN. 

% Scores
% In the class scores table for this task, it is shown that both the averaged Precision and Recall were very similar (Table \ref{Overall-Scores-8-Class-Oversampled}). 
Table \ref{Overall-Scores-8-Class-Regular} summarizes the results
for the f1-scores for all algorithms. 
We find that all 6 transient types found in the
6-Transient classification task had a f1-score decrease (Table
\ref{Overall-Scores-6-Transient-Regular}). 
Even the Other class f1-score is lower than in the 7-Transient classification task (Table
\ref{Overall-Scores-7-Transient-Regular}).
The Non-Transient f1-score is also worse when compared to the 7-Class
task (Table \ref{Overall-Scores-7-Class-Regular}).  


\import{./figures/8-Class/}{importances.tex}

Figure \ref{Importances-8-Class} displays the feature importance ranking.
This list ranks first stetson\textunderscore j with an 8\% importance,
followed by amp, sk, std, mad, with values around 6\%. 
The lowest raking features are the five high level polynomial: poly4\textunderscore
t1,  poly4\textunderscore t2, poly3\textunderscore t3,
poly4\textunderscore t3 and poly4\textunderscore t4. 


\subsection{Summary}

Among the different feature re-scaling techniques, Standardization
primes over Normalization in obtaining the highest f1-scores. 
For every single combination of parameters possible, the highest
scores are found with Standardization. 

Regarding the algorithms used, RF always achieves the highest scores,
followed by NN,  and finally SVM. 
SVMs only outperform NN  in the Binary classification task. 
Additionally,  the best classification scores come from more than 20
features. 
Furthermore, the highest scores are usually obtained when using light
curves with at least 10 observations. 
Finally, training with the unbalanced dataset frequently provides the
highest scores.  

In detail, machine learning algorithms behave similarly in some
cases: 

\begin{itemize}
    \item \textbf{Random forests} worked best when using all 30
      features. Light-curves with 10 observations provided the best
      results, except in binary classification where the highest
      results were obtained using 5 observations only. Finally, using
      balanced inputs worked the best only in the 7-Transient
      classification task.  
    \item \textbf{Neural Networks} scored their highest scores using
      26 or 30 features. Also, their highest scores were always
      obtained using light curves with at least 10 observations and
      unbalanced inputs. 
    \item \textbf{SVMs} frequently scored their highest scores when
      using more than 20 features and 10 observations minimum per
      light curve. Nonetheless, both unbalanced and balanced inputs
      were useful to obtain their highest scores. 
\end{itemize}

Each class behaves differently. 
In binary classification,
the best algorithm achieved a higher f1-score for Non-Transients,
followed closely by Transients. Furthermore, in multi-class
classification the best identified classes were by far: HPM and
Non-Transients. They were followed by AGN, CV and SN instances which
had lower performance, and followed at last by Blazars, Flares and
Other instances, with the worst classification performance.  

% An important majority of incorrectly classified instances were identified as either Supernovae or Non-Transients. A smaller amount was incorrectly classified as AGN and Other. 

Finally, a clear pattern regarding feature importance was found. In non-binary classification tasks, the best features were always: stetson\textunderscore j, sk, std, mad and amp. Those features obtained scores in the range 6 - 9, and stetson\textunderscore j always achieved the highest value. On the other hand, feature importance list show that high-level polynomial curve fitting features always obtained the lowest importance in those same tasks. Namely: poly4\textunderscore t1, poly4\textunderscore t2, poly3\textunderscore t3, poly4\textunderscore t3 and poly4\textunderscore t4, in that order.

Detailed results for each experiment are found in the following sub-sections.


\section{Conclusions}
%\import{./chapters/}{6.conc.tex}

The scope of forthcoming of large astronomical synaptic surveys such
as the LSST \citep{0805.2366} motivates the development and
exploration of automatized ways to detect transient sources.
In this paper we presented an approach for the automatic recognition
of transient events with machine learning techniques.   

The method we present is based on the study of light curves. 
We extracted its characteristic features to use them as inputs
to train three different machine learning algorithms: Random Forests,
Neural Networks and Support Vector Machines.
The features extracted from light curves were either statistical
descriptors of the observations, or polynomial curve fitting
coefficients applied to the light curves.   

The machine learning algorithms performed a variety of classification tasks.
Namely, binary classification among transients and non-transients, and
multi-class classification of various transient classes, sometimes
including non-transients too. Detailed description of these tasks can
be found in Section \ref{section_method}.  
Overall, the best classifier for all tasks was the Random Forest,
followed by Neural Networks and then Support Vector Machines. 

State of the art results were obtained when testing the trained models
to unseen sets of data, specifically in binary and transient
multi-class classification. 
Recall scores obtained from the best classifier for each task, are
comparable to those results found in \cite{1401.3211}
and \cite{1601.03931}.   


We also investigated the parameters that produced the best results.
Training with light curves which contained 10 observations minimum
generally outperformed those with minimum 5. 
Overall, the best results were also achieved using all 30 features per
light curve, while using 20 features performed worst. 
Such phenomena may be explained due to the lower noise that curves
with more observations provide.  
Since light-curves with higher observation counts may be better
defined, they could generate a more precise higher rank polynomial
fitting than non-transients, which were the newly proposed features
found in the 26 and 30 feature sets. 

In general, training with unbalanced-class data sets resulted in the
highest values. 
Contrary to what was expected, the usage of oversampled light-curves
may be a factor that biases the model during training. 
This could be explained if such artificial light curves were too
similar to the original ones, and thus the algorithms over-fitted
during training.  

We studied feature importance using Random Forests. 
The most important feature was always stetson\textunderscore j, followed by:
amplitude, standard deviation, skewness and median absolute
deviation. 
Conversely, 4th level polynomial fitting coefficients were
the ones which provided the least relevance, though using them was
still better, since the highest scores were a result of using all 30
features. 
Furthermore, lower polynomial fitting features like
poly2\textunderscore t1 and poly1\textunderscore t1 were proven to be
beneficial for classification, scoring much a higher importance. 

The above mentioned results demonstrate that the methodology works well.
Nevertheless, the final scores are similar to already published results.
This highlights the need to explore new directions to construct better
classifiers which are usable in next-generation astronomical surveys.   
As usual, a critical improvement can come from better observational
data.
An improved transient recognition could be achieved by training on
data with lower  amount of duplicates and errors, i.e. a more reliable
non-transient dataset.    
Expanding the methodology presented may be beneficial too. 
Using additional features could also increase the classifiers
performance. 
Moreover, different classification algorithms can also be tested for a
better detection of transient objects.  
Finally, testing with other more reliable oversampling types would be
beneficial for the purpose of working with limited datasets. 
While the oversampled/balanced inputs increased the performance of some of
the algorithms, different alternatives could improve upon the results
shown in this document.

We provide the code and the datasets that were used in this project.
The repository containing all this data may be found in the website \url{https://github.com/diegoalejogm/crts-transient-recognitionSection}. Additionally, Appendix \ref{section_code} contains a description of the
repository's structure.





\section*{Acknowledgements}

% The Acknowledgements section is not numbered. Here you can thank helpful
% colleagues, acknowledge funding agencies, telescopes and facilities used etc.
% Try to keep it short.
We thank Universidad de los Andes' call for project finalization for their economic support.
We also thank contributors and collaborators of the SciKit-Learn, Jupiter Notebooks and Pandas' Python libraries.
We thank Juan Pablo Reyes and Dominique Fouchez for helping with the research.
Finally, we want thank Dr. Andrew Drake for sharing with us the CRTS Transient dataset used in this project.

CRTS and CSDR2 are supported by the U.S.~National Science 
Foundation under grant NSF grants AST-1313422, AST-1413600, and 
AST-1518308.  The CSS survey is funded by the National Aeronautics
and Space Administration under Grant No. NNG05GF22G issued through
the Science Mission Directorate Near-Earth Objects Observations Program.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%%%%%%

% The best way to enter references is to use BibTeX:

\bibliographystyle{mnras}
\bibliography{bibliography}
%\bibliography{example} % if your bibtex file is called example.bib


% Alternatively you could enter them by hand, like this:
% This method is tedious and prone to error if you have lots of references
% \begin{thebibliography}{99}
% \bibitem[\protect\citeauthoryear{Author}{2012}]{Author2012}
% Author A.~N., 2013, Journal of Improbable Astronomy, 1, 1
% \bibitem[\protect\citeauthoryear{Others}{2013}]{Others2013}
% Others S., 2012, Journal of Interesting Stuff, 17, 198
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% APPENDICES %%%%%%%%%%%%%%%%%%%%%

\appendix
\section{Repository Description} \label{section_code}

The repository that contains the code, data and trained models we used in this project is found in the website \url{https://github.com/diegoalejogm/crts-transient-recognition}. For this project we used various python libraries: \texttt{jupyter} (1.0.0), \texttt{numpy} (1.14.3), \texttt{pandas} (0.22.0) and \texttt{scikit-learn} (0.19.1). Details other dependencies can be found in the included file \texttt{requirements.txt}.

We organized the repository in three main folders. The first one is \texttt{data}, and it contains the data we used in this project. We split data in three directories: \texttt{lightcurves}, \texttt{features} and \texttt{inputs}. The former contains pandas dataframes for raw and curated light-curves. The \texttt{features} subfolder contains the resulting features of pre-processing the light-curves, for both transients and non-transients. Finally, the \texttt{inputs} subfolder contains the already split training and testing inputs for the following hyper-parameter combinations: classification task, number of features, minimum number of observations, and balanced/unbalanced data. Each one of the input files contains a numpy tuple with the structure: \textit{(train features, train labels, test features, test labels)}

The second main folder in the repository is \texttt{notebooks}. It contains all the code used for this project. The jupyter notebooks contain the pipeline of our proposed experimental framework, and they are numbered in sequential order. Moreover, each notebook is documented within itself. Two additional non-numbered notebooks are used for exploration purposes. Conversely, the \texttt{.py} files are used to contain the python code which is used in the notebooks. They're separated by task type.

Finally, the \texttt{results} directory contains the products of running the code. One result type is named dataframes, which contains a pandas dataframe for each task. Each dataframe contains the testing and training results, including scores and confusion matrices.  On the other hand, the results sub-folder contains the feature importance list figures presented in this document.

For more information on the repository, make sure to read the README file included, or email us directly.

\section{Data-set's Statistics}

The tables \ref{Transient-Observation-Count} and \ref{Non-Transient-Observation-Count} in this annex contain statistical descriptors of the light curves' observation count, for transient and non-transient curves that contain at least 5 observations.
Table \ref{Transient-Observation-Count} makes reference to transient events, whereas table \ref{Non-Transient-Observation-Count} refers to non-transient objects. 
Each of these tables present the mean, standard deviation, percentiles (25, 50 and 75) and maximum number of observations per light curve found in each dataset.

\newpage
\pagebreak

\import{./figures/ObsCount/}{transient.tex}
\import{./figures/ObsCount/}{nontransient.tex}

\section{Task Experimentation Results}

The group of tables in this section contain the quantitative results of experimenting with the classification tasks. 

Tables \ref{Overall-Scores-Binary}, \ref{Overall-Scores-6-Transient-Regular}, \ref{Overall-Scores-7-Transient-Regular}, \ref{Overall-Scores-7-Class-Regular}, and  \ref{Overall-Scores-8-Class-Regular} describe the Precision, Recall and F1 Score, of every class, that the best performing algorithm achieved for each classification task. It also presents the number of test elements that were used in each class used to calculate such tables. 

Tables \ref{Confusion-Binary}, \ref{Confusion-6-Transient}, \ref{Confusion-7-Transient} \ref{Confusion-7-Class}, and  \ref{Confusion-8-Class} present the confusion matrix obtained when running the best performing algorithm on test data. 

\newpage
\pagebreak



\import{./figures/Binary/}{scores_reg.tex}
\import{./figures/6-Transient/}{scores_reg.tex}
\import{./figures/7-Transient/}{scores_reg.tex}
\import{./figures/7-Class/}{scores_reg.tex}
\import{./figures/8-Class/}{scores_reg.tex}

\import{./figures/Binary/}{cnf_matrix.tex}
\import{./figures/6-Transient/}{cnf_matrix.tex}
\import{./figures/7-Transient/}{cnf_matrix.tex}
\import{./figures/7-Class/}{cnf_matrix.tex}
\import{./figures/8-Class/}{cnf_matrix.tex}
% \section{Some extra material}

% If you want to present additional material which would interrupt the flow of the main paper,
% it can be placed in an Appendix which appears after the list of references.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Don't change these lines
% \bsp	% typesetting comment
\label{lastpage}
\end{document}

% End of mnras_template.tex
